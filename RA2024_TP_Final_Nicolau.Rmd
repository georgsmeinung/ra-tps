---
title: "Regresión Avanzada 2024"
subtitle: "Trabajo Práctico Final"
author: "Jorge Nicolau"
output:
   html_document:
     toc: yes
     code_folding: show
     toc_float: yes
     df_print: paged
     theme: united
     code_download: true
editor_options: 
  markdown: 
    wrap: none
---

```{r setup, include=FALSE}
# Limpieza del Workspace
rm(list = ls())

# Determinar el directorio del script actual
# y cambiar el working directory al directorio del script actual
library(rstudioapi)
script_full_path <- rstudioapi::getSourceEditorContext()$path
script_path <- file.path(dirname(script_full_path), "")
setwd(script_path)

knitr::opts_chunk$set(echo = TRUE)
# Deshabilita la impresión de tus resultados en notación científica
options(scipen=999)
```

# Preprocesamiento de Datos

Dado que no todos los componentes del dataset están en formato tabular se requiere la conversión para poder combinar los datos de las diferentes partes. Antes proceder con la combinación se pasa de formato JSON a CSV los archivos `train_fraud_labels.json` y `mcc_codes.json`.

El archivo resultante `train_fraud_labels.csv` servirá para la combinación de los datos de fraude con el dataset principal, mientras que `mcc_codes.csv` servirá para la conversión de los códigos de categoría de comercio a texto.

El código de conversión de JSON a CSV está escrito en Python porque maneja mejor la lectura de archivos grandes. Se sabe que el archivo JSON tienen una estructura simple con un solo nivel de anidamiento que dificulta la lectura con R por línea como se muesrta a continuación:

```{json}
{"target": {"10649266": "No", "23410063": "No", "9316588": "No", "12478022": "No", "9558530": "No", "12532830": "No", "19526714": "No", "9906964": "No", "13224888": "No", "13749094": "No", "12303776": "No", "19480376": "No", "11716050": "No", "20025400": "No", "7661688": "No", "16662807": "No", "21419778": "No", "18011186": "No", "23289598": "No", "11644547": "No", "23235120": "No", "19748218": "No", "8720720": "No", "18335831": "No", "18936727": "No", "15223870": "No"}}
```

La aproximación con lectura por bytes tampoco es eficiente, por lo que se decidió usar Python para la conversión del archivo `train_fraud_labels.json` de 151,7 MB de tamaño.

### Conversión de `train_fraud_labels.json` a `train_fraud_labels.csv`

```{python json_to_csv_fraud_labels}

# Dataset: Financial Transactions Dataset: Analytics
# Archivo principal con informacion de 13.305.915 transacciones en 1.2 GB
# Source: https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets 

import json
import csv
from tqdm import tqdm

def json_to_csv(json_data, output_csv):
    # Cargar los datos JSON
    with open(json_data, 'r') as json_file:
        data = json.load(json_file)

    # Asegurarse de que la clave 'target' esté en los datos JSON
    if 'target' not in data:
        raise ValueError("El JSON no contiene la clave 'target'")

    # Obtener los pares de ID y valor de 'target'
    target_data = data['target']

    # Escribir los datos en un archivo CSV
    with open(output_csv, 'w', newline='') as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(['transaction_id', 'fraud'])  # Escribir los encabezados

        for transaction_id, fraud in tqdm(target_data.items(), desc="Procesando registros", unit="registro"):
            writer.writerow([transaction_id, fraud])

# Ejemplo de uso
json_to_csv('train_fraud_labels.json', 'train_fraud_labels.csv')

```

La cantidad de registros en el archivo `train_fraud_labels.json` es de 8.914.963, inferior al archivo principal de transacciones con 13.305.915 registros dado que son etiquetas sólo para el conjunto de entrenamiento.

### Conversión de `mcc_codes.json` a `mcc_codes.csv`

Para este archivo se puede usar R para la conversión ya que es un archivo pequeño y simple.

```{r json_to_csv_fraud_labels}

library(jsonlite)
library(progress)
library(data.table)

# Función para convertir JSON a CSV usando data.table y mostrar progreso
convert_json_file_to_csv_with_progress <- function(input_json_file, output_csv_file) {
  # Leer el JSON desde el archivo
  json_data <- fromJSON(input_json_file)
  
  # Crear un data.table vacío para almacenar los resultados
  data <- data.table(Code = character(), Description = character())
  
  # Inicializar el progreso
  pb <- progress_bar$new(
    format = "  Procesando [:bar] :percent en :elapsed segundos",
    total = length(json_data),
    clear = FALSE,
    width = 60
  )
  
  # Iterar sobre los elementos del JSON
  for (code in names(json_data)) {
    pb$tick() # Actualizar el progreso
    data <- rbind(data, data.table(Code = code, Description = json_data[[code]]))
  }
  
  # Guardar el data.table como CSV
  fwrite(data, file = output_csv_file, row.names = FALSE)
  
  message("El archivo CSV se ha guardado como: ", output_csv_file)
}

# Llamar a la función con nombres de archivo
input_json_file <- "mcc_codes.json" # Cambia por el nombre de tu archivo JSON
output_csv_file <- "mcc_codes.csv" # Cambia por el nombre deseado del archivo CSV
convert_json_file_to_csv_with_progress(input_json_file, output_csv_file)

```

## Filtrar y Muestrear Datos de Transacciones

Una vez que se han convertido los archivos JSON a CSV, se procede a filtrar y muestrear los datos de transacciones para obtener un subconjunto que contenga la misma proporcion de estados y transsacciones fraudulentas que el conjunto original.

En primer lugar, se carga el dataset original y el de fraude, se unen por la columna correspondiente (el id de transacción).

```{r tx_data_fraud_match}
library(dplyr)
library(readr)

# Leer los datos
transactions_data <- read_csv("transactions_data.csv")
train_fraud_labels <- read_csv("train_fraud_labels.csv")

# Prefijo para las columnas del conjunto de fraude
colnames(train_fraud_labels) <- paste0("fd_", colnames(train_fraud_labels))

# Unir los datos por las columnas correspondientes
merged_data <- left_join(transactions_data, train_fraud_labels, 
                          by = c("id" = "fd_transaction_id"))
```

Luego, se calcula el tamaño de la muestra a obtener (2% del total) y se muestrea manteniendo las proporciones de estados y transacciones fraudulentas.

```{r sample_2pct_tx_data}

# Calcular el 2% del total
total_rows <- nrow(merged_data)
sample_size <- floor(total_rows * 0.02)

# Muestrear manteniendo proporciones
set.seed(999901)
sampled_data <- merged_data %>%
  group_by(merchant_state, fd_fraud) %>%
  sample_frac(min(sample_size / total_rows, 1))

# Guardar los datos muestreados
write_csv(sampled_data, "sampled_transactions.csv")

# Mostrar los datos resultantes
print(head(sampled_data))
```

## Construcción del Dataset Completo

Con la muestra del 2% del dataset de transacciones con etiquetas de fraude, se procede a unir los datos de georreferencias, códigos postales, tarjetas, usuarios y códigos de categoría de comercio (MCC) para construir un dataset completo que contenga toda la información necesaria para el análisis.

Primero se cargan los datos de georreferencias de estados de USA y se los une al dataset de transacciones por la abreviatura de estado (codigo de dos letras).

```{r merge_georef_states_data}
library(dplyr)
library(readr)

# Leer los datos de transacciones
transactions_data <- read_csv("sampled_transactions.csv")
print(str(transactions_data))

# Dataset: USA States (Generalized)
# Source https://public-data-hub-dhhs.hub.arcgis.com/datasets/usa-states-generalized/about
# Leer los datos de georreferencias de estados de USA
georef_data <- read_delim("usa_states_generalized.csv", delim = ",")
# Agregar un prefijo a los nombres de columnas para identificar 
# que son datos de georreferencias
colnames(georef_data) <- paste0("st_", colnames(georef_data))
# Pasar a minuscúlas todos los nombres de columnas
colnames(georef_data) <- tolower(colnames(georef_data))
# Convertir los espacios en nombres de columnas a guiones bajos 
colnames(georef_data) <- gsub(" ", "_", colnames(georef_data))
print(str(georef_data))

# Unir los datos por abreviatura de estado
merged_data <- transactions_data %>%
  left_join(georef_data, by = c("merchant_state" = "st_abbreviation"))
```

Luego, se unen los datos de códigos postales al dataset resultante.

```{r merge_zip_data}

# Dataset: US Zip Codes Points- United States of America
# Source: https://data.opendatasoft.com/explore/dataset/georef-united-states-of-america-zc-point%40public
# Leer los datos de códigos postales
zip_data <- read_delim("georef_usa_zc_point.csv", delim = ";")
# Agregar un prefijo a los nombres de columnas para identificar
# que son datos de la tabla de códigos postales
colnames(zip_data) <- paste0("zp_", colnames(zip_data))
# Pasar a minuscúlas todos los nombres de columnas
colnames(zip_data) <- tolower(colnames(zip_data))
# Convertir los espacios en nombres de columnas a guiones bajos 
colnames(zip_data) <- gsub(" ", "_", colnames(zip_data))
print(str(zip_data))

# Unir los datos por código postal
merged_data <- merged_data %>%
  left_join(zip_data, by = c("zip" = "zp_zip_code"))
```

A continuación, se unen los datos de tarjetas y usuarios al dataset resultante.

```{r merge_cards_users_data}

# Leer los datos de tarjetas
cards_data <- read_csv("cards_data.csv")
# Agregar un prefijo a los nombres de columnas para identificar
# que son datos de la tabla de tarjetas
colnames(cards_data) <- paste0("cd_", colnames(cards_data))
print(str(cards_data))

# Unir los datos por ID de tarjeta
merged_data <- merged_data %>%
  left_join(cards_data, by = c("card_id" = "cd_id"))

# Leer los datos de usuarios
users_data <- read_csv("users_data.csv")
# Agregar un prefijo a los nombres de columnas para identificar
# que son datos de la tabla de usuarios
colnames(users_data) <- paste0("ud_", colnames(users_data))
print(str(users_data))

# Unir los datos por ID de cliente
merged_data <- merged_data %>%
  left_join(users_data, by = c("client_id" = "ud_id"))

# Leer los datos de MCC
mcc_data <- read_csv("mcc_codes.csv")
# Agregar un prefijo a los nombres de columnas para identificar
# que son datos de la tabla de códigos de categoría de comercio
colnames(mcc_data) <- paste0("mcc_", colnames(mcc_data))
# Pasar a minuscúlas todos los nombres de columnas
colnames(mcc_data) <- tolower(colnames(mcc_data))
print(str(mcc_data))

# Unir los datos por MCC
merged_data <- merged_data %>%
  left_join(mcc_data, by = c("mcc" = "mcc_code"))

# Guardar los datos completos
write_csv(merged_data, "transactions_data_complete.csv")
```

El dataset completo resultante se guarda en un archivo CSV llamado `transactions_data_complete.csv` que contiene toda la información combinada de transacciones, georreferencias, códigos postales, tarjetas, usuarios y códigos de categoría de comercio con esta estructura:

```{r}
print(str(merged_data, list.len = Inf))
```

## Depuración de Columnas Redundantes

Previsiblemente luego de comoponer el dataset de diferentes fuentes se generan campos claramente redundantes que duplican información o podrían estar correlacionados, por lo que en base al análisis de los datos se procede a eliminar las columnas redundantes.

-   `client_id` y `cd_client_id`: Ambos representan al cliente, pero están presentes en dos prefijos distintos (cd\_ y sin prefijo). Es suficiente con `client_id`
-   `card_id` y `cd_card_number`: Si `card_id` es un identificador único y `cd_card_number` representa el número completo de la tarjeta, podrían estar relacionados. Por razones de privacidad y redundancia, se va a mantener solo uno `card_id`, para evitar difundir números de tarjetas de crédito aun cuando no estén activas.
-   `merchant_city` y `zp_official_usps_city_name`: Ambos indican la ciudad; se va a mantener solo `merchant_city`.
-   `merchant_state` y `zp_official_usps_state_code`: Ambos indican el estado; se va a mantener `merchant_state`.
-   `zp_official_state_name` y `st_name`: Ambos representan el nombre del estado; se mantiene solo `st_name`.
-   `st_population\_(2017)`, `st_pop2010`, `st_pop.\_per_sq.\_mi.`, `st_pop10_sqmi`: Hay múltiples columnas relacionadas con población. Se mantendrá `st_population\_(2017)` por ser la más reciente y se la renombrará a `st_population`.
-   `st_med_age`, `st_med_age_m`, `st_med_age_f`: Si solo se necesita la mediana de edad general, las otras dos columnas (por género) pueden ser redundantes; sin embargo, para analizar por género hábitos de consumo, podrían ser útiles.
-   `merchant_id` y `mcc_description`: Si el identificador merchant_id está siempre asociado a una única descripción mcc_description, esta última podría inferirse y no ser necesaria. Pero se mantendrá para facilitar la interpretación.
-   `zp_population` y `st_population\_(2017)`: Ambos representan población, pero uno es a nivel de ZCTA (ZIP Code Tabulation Areas o sea la localidad) y el otro a nivel estatal. Aunque podría ser redundante dependiendo del nivel de análisis se mantendrán ambos para permitir análisis a diferentes niveles de granularidad.

```{r remove_redundant_columns}

# Eliminar columnas redundantes
merged_data <- merged_data %>%
  select(-cd_client_id, -cd_card_number, -zp_official_usps_city_name, -zp_official_usps_state_code, 
         -zp_official_state_name, -st_pop2010, -st_pop._per_sq._mi., -st_pop10_sqmi)

# Renombrar columnas
colnames(merged_data) <- gsub("st_population_\\(2017\\)", "st_population", colnames(merged_data))
```

## Depuración de campos parcialmente redundantes o derivados

En estos casos los campos podrían estar ser derivados unos de otros, lo que podría implicar redundancia.

-   `st_males` y `st_females`: Si ya se tienen estas columnas el total de población (en el renombrado `st_population`) puede calcularse. Se mantendrán para análisis por género; `st_population` se mantendrá para facilitar el análisis general.
-   `st_white`, `st_black`, `st_ameri_es`, `st_asian`, `st_hawn_pi`, `st_hispanic`, `st_other`, `st_mult_race`: Dado que no se necesita analizar por grupo étnico se eliminan.
-   `zp_density` y `zp_population`: Si ya se tiene el área del ZCTA, la densidad puede calcularse: se eliminará `zp_density`.
-   `ud_birth_year` y `ud_current_age`: ud_birth_year y el año actual pueden calcular la edad, haciendo redundante `ud_current_age`.
-   `ud_latitude`,`ud_longitude`, `zp_timezone` y ` zp_geo_point` con respecto a ud_address: Si la dirección se geocodificó, estas columnas pueden ser redundantes. Se mantendrán para análisis de ubicación pero se eliminará `zp_geo_point` y `zp_timezone`.
-   `st_sqmi`,`st_shape__length` y `st_shape__area`: parecen medir el tamaño del estado; mantener uno es suficiente. Se mantendrá `st_sqmi` por ser más claro y se convertirá a kilómetros cuadradados renombrándola a `st_sqkm`.

```{r remove_partial_redundant_columns}

# Eliminar columnas parcialmente redundantes
merged_data <- merged_data %>%
  select(-zp_density, -ud_current_age, -st_shape__area, -st_shape__length, -zp_geo_point, -zp_timezone, 
         -st_white, -st_black, -st_ameri_es, -st_asian, -st_hawn_pi, -st_hispanic, -st_other)

# Convertir millas cuadradas a kilómetros cuadrados
merged_data$st_sqkm <- merged_data$st_sqmi * 2.58999

# Eliminar columnas de millas cuadradas
merged_data <- merged_data %>%
  select(-st_sqmi)
```

## Depuración de campos potencialmente no necesarios para el análisis

Algunos campos parecen específicos y pueden no ser útiles en todos los análisis:

-   `st_fips` (Federal Information Processing Standard), `zp_official_county_code`, `zp_primary_official_county_code`, `st_globalid`, `zp_zcta`, `zp_zcta_parent`, `zp_imprecise`, `zp_military`: Estos son identificadores administrativos que podrían no ser útiles para el análisis demografico.
-   `zp_county_weights`: Si no se está trabajando con pesos específicos, podría omitirse.
-   `cd_cvv`: Este campo no suele ser útil en análisis y puede representar un riesgo de seguridad.

```{r remove_unnecessary_columns}

# Eliminar columnas potencialmente no necesarias
merged_data <- merged_data %>%
  select(-st_fips, -zp_official_county_code, -zp_primary_official_county_code, -zp_county_weights, -cd_cvv, -st_globalid, -zp_zcta, -zp_zcta_parent, -zp_imprecise, -zp_military)
```

Se revisa la estructura del dataset limpio y se guardan los cambios en un archivo CSV.

```{r}
# Guardar los datos limpios
write_csv(merged_data, "transactions_data_clean.csv")

# Mostar la estructura del dataset limpio
print(str(merged_data, list.len = Inf))
```

# Análisis Exploratorio de Datos

Para realizar un análisis exploratorio de los datos, se cargan los datos completos y se realizan algunas visualizaciones y cálculos descriptivos.

```{r eda_transactions_data}

library(dplyr)
library(ggplot2)
library(readr)

# Leer los datos completos
transactions_data <- read_csv("transactions_data_complete.csv")

# Resumen de los datos
summary_data <- transactions_data %>%
  select_if(is.numeric) %>%
  summary()

# Visualización de la distribución de
# montos de transacciones por estado

# Visualización de la distribución de montos de transacciones
# por estado y fraude

# Visualización de la distribución de montos de transacciones
# por categoría de comercio

# Visualización de la distribución de montos de transacciones
# por categoría de comercio y fraude

# Visualización de la distribución de montos de transacciones
# por tipo de tarjeta

# Visualización de la distribución de montos de transacciones
# por tipo de tarjeta y fraude

# Visualización de la distribución de montos de transacciones
# por tipo de tarjeta y estado

# Visualización de la distribución de montos de transacciones
# por tipo de tarjeta y categoría de comercio

# Visualización de la distribución de montos de transacciones
# por tipo de tarjeta, categoría de comercio y estado

```
